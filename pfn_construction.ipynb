{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "50919dec",
   "metadata": {},
   "source": [
    "# From Scratch Implementation of Bernoulli-Beta transformer \n",
    "\n",
    "We implement a transformer that outputs the optimal bayes PPD. We do so through the following steps:\n",
    "\n",
    "1. Train a MLP to convert the sufficient statistics into the PPD\n",
    "2. **Explicitly** define an attention head that computes the sufficient statistics\n",
    "\n",
    "\n",
    "## Attention Head Implementation\n",
    "\n",
    "The sufficient statistics for the data is the tuple $(H,N)$ (or $(T,N)$ depending on the final token) where $H$ is the number of heads and $N$ is the length of the sequence. The attention head will compute these statistics from the input sequence. We suppose that $N$ is purely encoded via the positional embedding. Empirically we find that a gradient descent trained transformer implements a counting head, counting the observations of the *final* token in the sequence.\n",
    "\n",
    "\n",
    "## MLP Implementation\n",
    "\n",
    "The MLP is trained. Given an embedding vector that encodes the sufficient statistics, i.e \n",
    "\n",
    "$$\n",
    "E_{H,N} = f(H,N)\n",
    "$$\n",
    "\n",
    "the MLP outputs a vector corresponding to the PPD. We can form a linear map down to the log-odds, in which the solution is\n",
    "\n",
    "$$\n",
    "\\log \\left( \\frac{\\alpha + H}{\\beta + (N-H)}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8552af",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb0d33f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ce58c7",
   "metadata": {},
   "source": [
    "# Defining Variables\n",
    "\n",
    "A perhaps easy way to define our embeddings is to simply use a unit vector in euclidean space. However to show this works without a privileged basis we randomise our embedding vectors.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5440114c",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "\n",
    "def random_embedding(dim):\n",
    "    \"\"\"\n",
    "    Generate a random unit embedding vector of given dimension.\n",
    "    \"\"\"\n",
    "    vec = torch.randn(dim)\n",
    "    return vec / torch.norm(vec)  # normalize to unit vector\n",
    "\n",
    "pos_embed = random_embedding(embedding_dim) #this direction corresponds to length of the sequence, and is in the null space of the attention head\n",
    "head_embed = random_embedding(embedding_dim) #this direction corresponds to the head of the sequence\n",
    "tail_embed = random_embedding(embedding_dim) #this direction corresponds to the tail of the sequence\n",
    "bos_embed = random_embedding(embedding_dim) #this direction corresponds to the beginning of the sequence\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e68b160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residual(last_token, pos_embed, head_embed, tail_embed, sequence_length, n_heads_or_tails):\n",
    "    \"\"\"\n",
    "    Compute the residual vector for the last token in the sequence.\n",
    "    \"\"\"\n",
    "    if last_token == \"head\":\n",
    "        return pos_embed * sequence_length + head_embed * n_heads_or_tails\n",
    "    else:\n",
    "        return pos_embed * sequence_length + tail_embed * n_heads_or_tails\n",
    "\n",
    "\n",
    "def get_PPD(last_token, n_heads_or_tails, sequence_length, alpha, beta):\n",
    "    \"\"\"\n",
    "    Compute the PPD (Probability of Previous Distribution) for the last token.\n",
    "    \"\"\"\n",
    "    prob = (alpha + n_heads_or_tails) / (sequence_length + alpha + beta)\n",
    "    complement = 1 - prob \n",
    "\n",
    "    if last_token == \"head\":\n",
    "        return torch.tensor([0, prob, complement])\n",
    "    else:\n",
    "        return torch.tensor([0, complement, prob]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a8f553",
   "metadata": {},
   "source": [
    "# Training the MLP\n",
    "\n",
    "MLP takes in residual vector, and outputs logits corresponding to the PPD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "676c6b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_factor=4):\n",
    "        super().__init__()\n",
    "        self.layer1 = torch.nn.Linear(embedding_dim, embedding_dim * hidden_factor)\n",
    "        self.layer2 = torch.nn.Linear(embedding_dim * hidden_factor, 3)  # Output logits for head and tail\n",
    "        self.activation = torch.nn.ReLU()\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.activation(self.layer1(x))\n",
    "        return self.softmax(self.layer2(x))\n",
    "\n",
    "# Initialize the MLP\n",
    "mlp = MLP(embedding_dim)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(mlp.parameters(), lr=0.001)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "context-vs-finetune",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
